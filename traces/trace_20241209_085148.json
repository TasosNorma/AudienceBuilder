{
  "url": "https://techcrunch.com/2024/12/08/apple-sued-over-abandoning-csam-detection-for-icloud/",
  "start_time": "2024-12-09T08:50:50.674473",
  "steps": [
    {
      "name": "extract_article",
      "duration": 5.969629764556885,
      "success": true
    },
    {
      "name": "get_secondary_articles",
      "duration": 47.59338092803955,
      "success": true
    },
    {
      "name": "run_chain",
      "duration": 4.610602140426636,
      "success": true
    },
    {
      "name": "parse_tweets",
      "duration": 3.981590270996094e-05,
      "success": true
    }
  ],
  "content": {
    "article_preview": "Apple sued over abandoning CSAM detection for iCloud\n\nAnthony Ha\n\n10:26 AM PST \u00b7 December 8, 2024\n\nApple is being sued over its decision not to implement a system that would have scanned iCloud photos for child sexual abuse material (CSAM). The lawsuit argues that by not doing more to prevent the spread of this material, it\u2019s forcing victims to relive their trauma, according to The New York Times. The suit describes Apple as announcing 'a widely touted improved design aimed at protecting children,' then failing to 'implement those designs or take any measures to detect and limit' this material.\n\nApple first announced the system in 2021, explaining that it would use digital signatures from the National Center for Missing and Exploited Children and other groups to detect known CSAM content in users\u2019 iCloud libraries. However, it appeared to abandon those plans after security and privacy advocates suggested they could create a backdoor for government surveillance.\n\nThe lawsuit reportedly comes from a 27-year-old woman who is suing Apple under a pseudonym. She said a relative molested her when she was an infant and shared images of her online, and that she still receives law enforcement notices nearly every day about someone being charged over possessing those images. Attorney James Marsh, who is involved with the lawsuit, said there\u2019s a potential group of 2,680 victims who could be entitled to compensation in this case.\n\nTechCrunch has reached out to Apple for comment. A company spokesperson told The Times Apple is 'urgently and actively innovating to combat these crimes without compromising the security and privacy of all our users.' In August, a 9-year-old girl and her guardian sued Apple, accusing the company of failing to address CSAM on iCloud.",
    "secondary_articles": "**Article: 1**\nURL: https://www.nytimes.com/2024/12/08/technology/apple-child-sexual-abuse-material-lawsuit.html\nContent: \n\n---\n\n**Article: 2**\nURL: https://techcrunch.com/2021/08/05/apple-icloud-photos-scanning/\nContent: Later this year, Apple will roll out a technology that will allow the company to detect and report known child sexual abuse material to law enforcement in a way it says will preserve user privacy.\n\nThe detection of child sexual abuse material (CSAM) is one of several new features aimed at better protecting the children who use its services from online harm, including filters to block potentially sexually explicit photos sent and received through a child\u2019s iMessage account.\n\nApple's new CSAM detection technology \u2014 NeuralHash \u2014 works on a user\u2019s device, identifying if a user uploads known child abuse imagery to iCloud without decrypting the images until a threshold is met.\n\nNeuralHash uses a cryptographic technique called private set intersection to detect a hash match without revealing what the image is or alerting the user, ensuring a one in one trillion chance of a false positive.\n\nDespite the wide support of efforts to combat child sexual abuse, there is still a component of surveillance that many would feel uncomfortable handing over to an algorithm, and some security experts are calling for more public discussion before Apple rolls the technology out to users.\n\n---\n\n**Article: 3**\nURL: https://techcrunch.com/2021/08/10/interview-apples-head-of-privacy-details-child-abuse-detection-and-messages-safety-features/\nContent: Last week, Apple announced a series of new features targeted at child safety on its devices. Though not live yet, the features will arrive later this year for users. Though the goals of these features are universally accepted to be good ones \u2014 the protection of minors and the limit of the spread of Child Sexual Abuse Material (CSAM), there have been some questions about the methods Apple is using.\n\nApple is announcing three different things here, some of which are being confused with one another in coverage and in the minds of the public.\n\nCSAM detection in iCloud Photos \u2013 A detection system called NeuralHash creates identifiers it can compare with IDs from the National Center for Missing and Exploited Children and other entities to detect known CSAM content in iCloud Photo libraries.\n\nCommunication Safety in Messages \u2013 A feature that a parent opts to turn on for a minor on their iCloud Family account.\n\nInterventions in Siri and search \u2013 A feature that will intervene when a user tries to search for CSAM-related terms through Siri and search and will inform the user of the intervention and offer resources.\n\nThere have also been questions about the on-device hashing of photos to create identifiers that can be compared with the database.\n\nThough NeuralHash is a technology that can be used for other kinds of features like faster search in photos, it\u2019s not currently used for anything else on iPhone aside from CSAM detection.\n\nWhen iCloud Photos is disabled, the feature stops working completely.\n\nThe system as designed doesn\u2019t reveal the result of the match to the device or to Apple.\n\nApple is unable to process individual vouchers; instead, all the properties of our system mean that it\u2019s only once an account has accumulated a collection of vouchers associated with illegal, known CSAM images that we are able to learn anything about the user\u2019s account.\n\nGovernments and agencies worldwide are constantly pressuring all large organizations that have any sort of end-to-end or even partial encryption enabled for their users.\n\nIs launching the feature and this capability with on-device hash matching an effort to stave off those requests and say, look, we can provide you with the information that you require to track down and prevent CSAM activity \u2014 but without compromising a user\u2019s privacy?\n\nOur system involves both an on-device component where the voucher is created, but nothing is learned, and a server-side component, which is where that voucher is sent along with data coming to Apple service and processed across the account to learn if there are collections of illegal CSAM. That means that it is a service feature. I understand that it\u2019s a complex attribute that a feature of the service has a portion where the voucher is generated on the device, but again, nothing\u2019s learned about the content on the device. The voucher generation is actually exactly what enables us not to have to begin processing all users\u2019 content on our servers, which we\u2019ve never done for iCloud Photos.\n\nOne of the bigger queries about this system is that Apple has said that it will just refuse action if it is asked by a government or other agency to compromise by adding things that are not CSAM to the database to check for them on-device. There are some examples where Apple has had to comply with local law at the highest levels if it wants to operate there, China being an example. So how do we trust that Apple is going to hew to this rejection of interference if pressured or asked by a government to compromise the system?\n\nWell first, that is launching only for U.S., iCloud accounts, and so the hypotheticals seem to bring up generic countries or other countries that aren\u2019t the U.S. when they speak in that way, and the therefore it seems to be the case that people agree U.S. law doesn\u2019t offer these kinds of capabilities to our government. But even in the case where we\u2019re talking about some attempt to change the system, it has a number of protections built in that make it not very useful for trying to identify individuals holding specifically objectionable images.\n\nThe hash list is built into the operating system, we have one global operating system and don\u2019t have the ability to target updates to individual users and so hash lists will be shared by all users when the system is enabled. And secondly, the system requires the threshold of images to be exceeded so trying to seek out even a single image from a person\u2019s device or set of people\u2019s devices won\u2019t work because the system simply does not provide any knowledge to Apple for single photos stored in our service.\n\nAnd then, thirdly, the system has built into it a stage of manual review where, if an account is flagged with a collection of illegal CSAM material, an Apple team will review that to make sure that it is a correct match of illegal CSAM material prior to making any referral to any external entity.\n\nAnd the last point that I would just add is that it does still preserve user choice, if a user does not like this kind of functionality, they can choose not to use iCloud Photos and if iCloud Photos is not enabled no part of the system is functional.\n\nIf users are not using iCloud Photos, NeuralHash will not run and will not generate any vouchers. CSAM detection is a neural hash being compared against a database of the known CSAM hashes that are part of the operating system image. None of that piece, nor any of the additional parts including the creation of the safety vouchers or the uploading of vouchers to iCloud Photos, is functioning if you\u2019re not using iCloud Photos.\n\nIn recent years, Apple has often leaned into the fact that on-device processing preserves user privacy. And in nearly every previous case I can think of, that\u2019s true. Scanning photos to identify their content and allow me to search them, for instance. I\u2019d rather that be done locally and never sent to a server.\n\nHowever, in this case, it seems like there may actually be a sort of anti-effect in that you\u2019re scanning locally, but for external use cases, rather than scanning for personal use \u2014 creating a \u2018less trust\u2019 scenario in the minds of some users. Add to this that every other cloud provider scans it on their servers and the question becomes why should this implementation being different from most others engender more trust in the user rather than less?\n\nI think we\u2019re raising the bar, compared to the industry standard way to do this. Any sort of server-side algorithm that\u2019s processing all users\u2019 photos is putting that data at more risk of disclosure and is, by definition, less transparent in terms of what it\u2019s doing on top of the user\u2019s library.\n\nSo, by building this into our operating system, we gain the same properties that the integrity of the operating system provides already across so many other features, the one global operating system that\u2019s the same for all users who download it and install it, and so it in one property is much more challenging, even how it would be targeted to an individual user.\n\nSecondly, you point out how use of on-device technology is privacy preserving, and in this case, that\u2019s a representation that I would make to you, again. That it\u2019s really the alternative to where users\u2019 libraries have to be processed on a server that is less private.\n\nThe things that we can say with this system is that it leaves privacy completely undisturbed for every other user who\u2019s not into this illegal behavior, Apple gains no additional knowledge about any users cloud library.\n\nCan this CSAM detection feature stay holistic when the device is physically compromised? Sometimes cryptography gets bypassed locally, somebody has the device in hand \u2014 are there any additional layers there?\n\nI think it\u2019s important to underscore how very challenging and expensive and rare this is. It\u2019s not a practical concern for most users, though it\u2019s one we take very seriously, because the protection of data on the device is paramount for us.\n\nWhy is there a threshold of images for reporting, isn\u2019t one piece of CSAM content too many? We want to ensure that the reports that we make to NCMEC are high-value and actionable, and one of the notions of all systems is that there\u2019s some uncertainty built in to whether or not that image matched.\n\nAnd so the threshold allows us to reach that point where we expect a false reporting rate for review of one in 1 trillion accounts per year. So, working against the idea that we do not have any interest in looking through users\u2019 photo libraries outside those that are holding collections of known CSAM the threshold allows us to have high confidence that those accounts that we review are ones that when we refer to NCMEC, law enforcement will be able to take up and effectively investigate, prosecute and convict.\n\n---\n\n**Article: 4**\nURL: https://techcrunch.com/2021/08/18/apples-csam-detection-tech-is-under-fire-again/\nContent: Apple has encountered monumental backlash to a new child sexual abuse material (CSAM) detection technology it announced earlier this month.\n\nThe system, which Apple calls NeuralHash, has yet to be activated for its billion-plus users, but the technology is already facing heat from security researchers who say the algorithm is producing flawed results.\n\nNeuralHash is designed to identify known CSAM on a user\u2019s device without having to possess the image or knowing the contents of the image.\n\nApple does this by looking for images on a user\u2019s device that have the same hash \u2014 a string of letters and numbers that can uniquely identify an image \u2014 that are provided by child protection organizations like NCMEC.\n\nIf NeuralHash finds 30 or more matching hashes, the images are flagged to Apple for a manual review before the account owner is reported to law enforcement.\n\nBut security experts and privacy advocates have expressed concern that the system could be abused by highly resourced actors, like governments, to implicate innocent victims or to manipulate the system to detect other materials that authoritarian nation states find objectionable.\n\nNCMEC called critics the 'screeching voices of the minority,' according to a leaked memo distributed internally to Apple staff.\n\nLast night, Asuhariet Ygvar reverse-engineered Apple\u2019s NeuralHash into a Python script and published code to GitHub, allowing anyone to test the technology regardless of whether they have an Apple device to test.\n\nIt didn\u2019t take long before others tinkered with the published code and soon came the first reported case of a 'hash collision,' which in NeuralHash\u2019s case is where two entirely different images produce the same hash.\n\nKenneth White, a cryptography expert, said in a tweet, 'I think some people aren\u2019t grasping that the time between the iOS NeuralHash code being found and the first collision was not months or days, but a couple of hours.'\n\nA senior lawmaker in the German parliament sent a letter to Apple chief executive Tim Cook this week saying that the company is walking down a 'dangerous path' and urged Apple not to implement the system.\n\n---\n\n**Article: 5**\nURL: https://www.law.com/therecorder/2024/08/14/irreversible-suffering-apple-hit-with-class-action-over-lack-of-safety-guardrails-for-children/?slreturn=20241208141809\nContent: 'Irreversible Suffering': Apple Hit With Class Action Over Lack of Safety 'Guardrails' for Children\n\nApple is facing a class action lawsuit filed in the U.S. District Court for the Northern District of California, alleging violations of sex-trafficking laws and consumer protection laws. The suit, initiated by the Buche Law Firm and Eisenberg & Baum, represents a 9-year-old child and her guardian, claiming that the child was a victim of child sexual abuse facilitated by Apple's iCloud platform. The complaint accuses Apple of failing to implement necessary detection tools for child sexual abuse material (CSAM), thereby neglecting its responsibility to protect vulnerable users.\n\nThe lawsuit claims that Apple has engaged in 'privacy-washing,' a deceptive marketing tactic that emphasizes consumer privacy while failing to implement effective safety measures. The plaintiffs argue that Apple's inaction has contributed to the proliferation of CSAM on its platform, raising significant concerns about the safety of children using its services.\n\n---\n",
    "prompt_template": "First template: You are a professional Twitter writer who focuses on business, data analytics, AI, and related news. Your goal is to craft engaging Twitter threads that attract and inform your audience, ultimately building a following for future entrepreneurial ventures.\r\n\r\n**Thread Structure & Guidelines:**\r\n\r\n1. **Hook (First Tweet):**\r\n    - **Purpose:** Grab attention and entice readers. Make them understand it's about news and not personal opinions.\r\n    - **Content:** Start with a bold statement or a compelling question related to the main topic. Be objective and highlight the key value proposition or benefit.\r\n    - **Length:** Keep under 280 characters.\r\n    - **Style:** Use concise and impactful language.\r\n2. **Body (Middle Tweets):**\r\n    - **Purpose:** Deliver key information and insights.\r\n    - **Content:** Break down the main points from the articles into digestible pieces. Incorporate relevant data, quotes, or statistics to add value.\r\n    - **Flow:** Ensure a natural progression between tweets for readability.\r\n    - **Length:** Each tweet should be under 280 characters.\r\n    - **Style:** Maintain clear and readable sentences.\r\n3. **Call-to-Action (Final Tweet):**\r\n    - **Purpose:** Drive engagement.\r\n    - **Content:** Include a clear directive that encourages interaction, such as asking for thoughts or suggesting to follow for more insights.\r\n    - **Alignment:** Make sure it aligns with the thread's content.\r\n    - **Length:** Under 280 characters.\r\n\r\n**Additional Guidelines:**\r\n\r\n- **Tone:** Maintain a professional and informative voice while staying approachable. Focus on factual news reporting rather than sensationalism, particularly in the opening tweet.\r\n- **Audience Focus:** Tailor content to professionals interested in business, data analytics, and AI.\r\n- **Language:** Avoid jargon; keep it accessible to a broad audience.\r\n- **Originality:** While summarizing the articles, ensure the content is original and offers a unique perspective. \r\n- **Facts-Based:** Don't comment with opinions, focus mainly on the facts that you can find in the articles. \n ### Suffix_   \n\nYou will be provided with one main article and additional secondary articles. The secondary articles are links referenced in the primary article.\nYour task is to write a compelling Twitter thread about the main article, enriching it with insights and context from the secondary articles. The thread should be informative, engaging, and encourage audience interaction.\n\n** Primary Article **\nThis is the primary article:\n{primary}\n\n\n** Secondary Articles **\nThese are the secondary articles.\n{secondary}\n",
    "final_prompt": "First template: You are a professional Twitter writer who focuses on business, data analytics, AI, and related news. Your goal is to craft engaging Twitter threads that attract and inform your audience, ultimately building a following for future entrepreneurial ventures.\r\n\r\n**Thread Structure & Guidelines:**\r\n\r\n1. **Hook (First Tweet):**\r\n    - **Purpose:** Grab attention and entice readers. Make them understand it's about news and not personal opinions.\r\n    - **Content:** Start with a bold statement or a compelling question related to the main topic. Be objective and highlight the key value proposition or benefit.\r\n    - **Length:** Keep under 280 characters.\r\n    - **Style:** Use concise and impactful language.\r\n2. **Body (Middle Tweets):**\r\n    - **Purpose:** Deliver key information and insights.\r\n    - **Content:** Break down the main points from the articles into digestible pieces. Incorporate relevant data, quotes, or statistics to add value.\r\n    - **Flow:** Ensure a natural progression between tweets for readability.\r\n    - **Length:** Each tweet should be under 280 characters.\r\n    - **Style:** Maintain clear and readable sentences.\r\n3. **Call-to-Action (Final Tweet):**\r\n    - **Purpose:** Drive engagement.\r\n    - **Content:** Include a clear directive that encourages interaction, such as asking for thoughts or suggesting to follow for more insights.\r\n    - **Alignment:** Make sure it aligns with the thread's content.\r\n    - **Length:** Under 280 characters.\r\n\r\n**Additional Guidelines:**\r\n\r\n- **Tone:** Maintain a professional and informative voice while staying approachable. Focus on factual news reporting rather than sensationalism, particularly in the opening tweet.\r\n- **Audience Focus:** Tailor content to professionals interested in business, data analytics, and AI.\r\n- **Language:** Avoid jargon; keep it accessible to a broad audience.\r\n- **Originality:** While summarizing the articles, ensure the content is original and offers a unique perspective. \r\n- **Facts-Based:** Don't comment with opinions, focus mainly on the facts that you can find in the articles. \n ### Suffix_   \n\nYou will be provided with one main article and additional secondary articles. The secondary articles are links referenced in the primary article.\nYour task is to write a compelling Twitter thread about the main article, enriching it with insights and context from the secondary articles. The thread should be informative, engaging, and encourage audience interaction.\n\n** Primary Article **\nThis is the primary article:\nApple sued over abandoning CSAM detection for iCloud\n\nAnthony Ha\n\n10:26 AM PST \u00b7 December 8, 2024\n\nApple is being sued over its decision not to implement a system that would have scanned iCloud photos for child sexual abuse material (CSAM). The lawsuit argues that by not doing more to prevent the spread of this material, it\u2019s forcing victims to relive their trauma, according to The New York Times. The suit describes Apple as announcing 'a widely touted improved design aimed at protecting children,' then failing to 'implement those designs or take any measures to detect and limit' this material.\n\nApple first announced the system in 2021, explaining that it would use digital signatures from the National Center for Missing and Exploited Children and other groups to detect known CSAM content in users\u2019 iCloud libraries. However, it appeared to abandon those plans after security and privacy advocates suggested they could create a backdoor for government surveillance.\n\nThe lawsuit reportedly comes from a 27-year-old woman who is suing Apple under a pseudonym. She said a relative molested her when she was an infant and shared images of her online, and that she still receives law enforcement notices nearly every day about someone being charged over possessing those images. Attorney James Marsh, who is involved with the lawsuit, said there\u2019s a potential group of 2,680 victims who could be entitled to compensation in this case.\n\nTechCrunch has reached out to Apple for comment. A company spokesperson told The Times Apple is 'urgently and actively innovating to combat these crimes without compromising the security and privacy of all our users.' In August, a 9-year-old girl and her guardian sued Apple, accusing the company of failing to address CSAM on iCloud.\n\n\n** Secondary Articles **\nThese are the secondary articles.\n**Article: 1**\nURL: https://www.nytimes.com/2024/12/08/technology/apple-child-sexual-abuse-material-lawsuit.html\nContent: \n\n---\n\n**Article: 2**\nURL: https://techcrunch.com/2021/08/05/apple-icloud-photos-scanning/\nContent: Later this year, Apple will roll out a technology that will allow the company to detect and report known child sexual abuse material to law enforcement in a way it says will preserve user privacy.\n\nThe detection of child sexual abuse material (CSAM) is one of several new features aimed at better protecting the children who use its services from online harm, including filters to block potentially sexually explicit photos sent and received through a child\u2019s iMessage account.\n\nApple's new CSAM detection technology \u2014 NeuralHash \u2014 works on a user\u2019s device, identifying if a user uploads known child abuse imagery to iCloud without decrypting the images until a threshold is met.\n\nNeuralHash uses a cryptographic technique called private set intersection to detect a hash match without revealing what the image is or alerting the user, ensuring a one in one trillion chance of a false positive.\n\nDespite the wide support of efforts to combat child sexual abuse, there is still a component of surveillance that many would feel uncomfortable handing over to an algorithm, and some security experts are calling for more public discussion before Apple rolls the technology out to users.\n\n---\n\n**Article: 3**\nURL: https://techcrunch.com/2021/08/10/interview-apples-head-of-privacy-details-child-abuse-detection-and-messages-safety-features/\nContent: Last week, Apple announced a series of new features targeted at child safety on its devices. Though not live yet, the features will arrive later this year for users. Though the goals of these features are universally accepted to be good ones \u2014 the protection of minors and the limit of the spread of Child Sexual Abuse Material (CSAM), there have been some questions about the methods Apple is using.\n\nApple is announcing three different things here, some of which are being confused with one another in coverage and in the minds of the public.\n\nCSAM detection in iCloud Photos \u2013 A detection system called NeuralHash creates identifiers it can compare with IDs from the National Center for Missing and Exploited Children and other entities to detect known CSAM content in iCloud Photo libraries.\n\nCommunication Safety in Messages \u2013 A feature that a parent opts to turn on for a minor on their iCloud Family account.\n\nInterventions in Siri and search \u2013 A feature that will intervene when a user tries to search for CSAM-related terms through Siri and search and will inform the user of the intervention and offer resources.\n\nThere have also been questions about the on-device hashing of photos to create identifiers that can be compared with the database.\n\nThough NeuralHash is a technology that can be used for other kinds of features like faster search in photos, it\u2019s not currently used for anything else on iPhone aside from CSAM detection.\n\nWhen iCloud Photos is disabled, the feature stops working completely.\n\nThe system as designed doesn\u2019t reveal the result of the match to the device or to Apple.\n\nApple is unable to process individual vouchers; instead, all the properties of our system mean that it\u2019s only once an account has accumulated a collection of vouchers associated with illegal, known CSAM images that we are able to learn anything about the user\u2019s account.\n\nGovernments and agencies worldwide are constantly pressuring all large organizations that have any sort of end-to-end or even partial encryption enabled for their users.\n\nIs launching the feature and this capability with on-device hash matching an effort to stave off those requests and say, look, we can provide you with the information that you require to track down and prevent CSAM activity \u2014 but without compromising a user\u2019s privacy?\n\nOur system involves both an on-device component where the voucher is created, but nothing is learned, and a server-side component, which is where that voucher is sent along with data coming to Apple service and processed across the account to learn if there are collections of illegal CSAM. That means that it is a service feature. I understand that it\u2019s a complex attribute that a feature of the service has a portion where the voucher is generated on the device, but again, nothing\u2019s learned about the content on the device. The voucher generation is actually exactly what enables us not to have to begin processing all users\u2019 content on our servers, which we\u2019ve never done for iCloud Photos.\n\nOne of the bigger queries about this system is that Apple has said that it will just refuse action if it is asked by a government or other agency to compromise by adding things that are not CSAM to the database to check for them on-device. There are some examples where Apple has had to comply with local law at the highest levels if it wants to operate there, China being an example. So how do we trust that Apple is going to hew to this rejection of interference if pressured or asked by a government to compromise the system?\n\nWell first, that is launching only for U.S., iCloud accounts, and so the hypotheticals seem to bring up generic countries or other countries that aren\u2019t the U.S. when they speak in that way, and the therefore it seems to be the case that people agree U.S. law doesn\u2019t offer these kinds of capabilities to our government. But even in the case where we\u2019re talking about some attempt to change the system, it has a number of protections built in that make it not very useful for trying to identify individuals holding specifically objectionable images.\n\nThe hash list is built into the operating system, we have one global operating system and don\u2019t have the ability to target updates to individual users and so hash lists will be shared by all users when the system is enabled. And secondly, the system requires the threshold of images to be exceeded so trying to seek out even a single image from a person\u2019s device or set of people\u2019s devices won\u2019t work because the system simply does not provide any knowledge to Apple for single photos stored in our service.\n\nAnd then, thirdly, the system has built into it a stage of manual review where, if an account is flagged with a collection of illegal CSAM material, an Apple team will review that to make sure that it is a correct match of illegal CSAM material prior to making any referral to any external entity.\n\nAnd the last point that I would just add is that it does still preserve user choice, if a user does not like this kind of functionality, they can choose not to use iCloud Photos and if iCloud Photos is not enabled no part of the system is functional.\n\nIf users are not using iCloud Photos, NeuralHash will not run and will not generate any vouchers. CSAM detection is a neural hash being compared against a database of the known CSAM hashes that are part of the operating system image. None of that piece, nor any of the additional parts including the creation of the safety vouchers or the uploading of vouchers to iCloud Photos, is functioning if you\u2019re not using iCloud Photos.\n\nIn recent years, Apple has often leaned into the fact that on-device processing preserves user privacy. And in nearly every previous case I can think of, that\u2019s true. Scanning photos to identify their content and allow me to search them, for instance. I\u2019d rather that be done locally and never sent to a server.\n\nHowever, in this case, it seems like there may actually be a sort of anti-effect in that you\u2019re scanning locally, but for external use cases, rather than scanning for personal use \u2014 creating a \u2018less trust\u2019 scenario in the minds of some users. Add to this that every other cloud provider scans it on their servers and the question becomes why should this implementation being different from most others engender more trust in the user rather than less?\n\nI think we\u2019re raising the bar, compared to the industry standard way to do this. Any sort of server-side algorithm that\u2019s processing all users\u2019 photos is putting that data at more risk of disclosure and is, by definition, less transparent in terms of what it\u2019s doing on top of the user\u2019s library.\n\nSo, by building this into our operating system, we gain the same properties that the integrity of the operating system provides already across so many other features, the one global operating system that\u2019s the same for all users who download it and install it, and so it in one property is much more challenging, even how it would be targeted to an individual user.\n\nSecondly, you point out how use of on-device technology is privacy preserving, and in this case, that\u2019s a representation that I would make to you, again. That it\u2019s really the alternative to where users\u2019 libraries have to be processed on a server that is less private.\n\nThe things that we can say with this system is that it leaves privacy completely undisturbed for every other user who\u2019s not into this illegal behavior, Apple gains no additional knowledge about any users cloud library.\n\nCan this CSAM detection feature stay holistic when the device is physically compromised? Sometimes cryptography gets bypassed locally, somebody has the device in hand \u2014 are there any additional layers there?\n\nI think it\u2019s important to underscore how very challenging and expensive and rare this is. It\u2019s not a practical concern for most users, though it\u2019s one we take very seriously, because the protection of data on the device is paramount for us.\n\nWhy is there a threshold of images for reporting, isn\u2019t one piece of CSAM content too many? We want to ensure that the reports that we make to NCMEC are high-value and actionable, and one of the notions of all systems is that there\u2019s some uncertainty built in to whether or not that image matched.\n\nAnd so the threshold allows us to reach that point where we expect a false reporting rate for review of one in 1 trillion accounts per year. So, working against the idea that we do not have any interest in looking through users\u2019 photo libraries outside those that are holding collections of known CSAM the threshold allows us to have high confidence that those accounts that we review are ones that when we refer to NCMEC, law enforcement will be able to take up and effectively investigate, prosecute and convict.\n\n---\n\n**Article: 4**\nURL: https://techcrunch.com/2021/08/18/apples-csam-detection-tech-is-under-fire-again/\nContent: Apple has encountered monumental backlash to a new child sexual abuse material (CSAM) detection technology it announced earlier this month.\n\nThe system, which Apple calls NeuralHash, has yet to be activated for its billion-plus users, but the technology is already facing heat from security researchers who say the algorithm is producing flawed results.\n\nNeuralHash is designed to identify known CSAM on a user\u2019s device without having to possess the image or knowing the contents of the image.\n\nApple does this by looking for images on a user\u2019s device that have the same hash \u2014 a string of letters and numbers that can uniquely identify an image \u2014 that are provided by child protection organizations like NCMEC.\n\nIf NeuralHash finds 30 or more matching hashes, the images are flagged to Apple for a manual review before the account owner is reported to law enforcement.\n\nBut security experts and privacy advocates have expressed concern that the system could be abused by highly resourced actors, like governments, to implicate innocent victims or to manipulate the system to detect other materials that authoritarian nation states find objectionable.\n\nNCMEC called critics the 'screeching voices of the minority,' according to a leaked memo distributed internally to Apple staff.\n\nLast night, Asuhariet Ygvar reverse-engineered Apple\u2019s NeuralHash into a Python script and published code to GitHub, allowing anyone to test the technology regardless of whether they have an Apple device to test.\n\nIt didn\u2019t take long before others tinkered with the published code and soon came the first reported case of a 'hash collision,' which in NeuralHash\u2019s case is where two entirely different images produce the same hash.\n\nKenneth White, a cryptography expert, said in a tweet, 'I think some people aren\u2019t grasping that the time between the iOS NeuralHash code being found and the first collision was not months or days, but a couple of hours.'\n\nA senior lawmaker in the German parliament sent a letter to Apple chief executive Tim Cook this week saying that the company is walking down a 'dangerous path' and urged Apple not to implement the system.\n\n---\n\n**Article: 5**\nURL: https://www.law.com/therecorder/2024/08/14/irreversible-suffering-apple-hit-with-class-action-over-lack-of-safety-guardrails-for-children/?slreturn=20241208141809\nContent: 'Irreversible Suffering': Apple Hit With Class Action Over Lack of Safety 'Guardrails' for Children\n\nApple is facing a class action lawsuit filed in the U.S. District Court for the Northern District of California, alleging violations of sex-trafficking laws and consumer protection laws. The suit, initiated by the Buche Law Firm and Eisenberg & Baum, represents a 9-year-old child and her guardian, claiming that the child was a victim of child sexual abuse facilitated by Apple's iCloud platform. The complaint accuses Apple of failing to implement necessary detection tools for child sexual abuse material (CSAM), thereby neglecting its responsibility to protect vulnerable users.\n\nThe lawsuit claims that Apple has engaged in 'privacy-washing,' a deceptive marketing tactic that emphasizes consumer privacy while failing to implement effective safety measures. The plaintiffs argue that Apple's inaction has contributed to the proliferation of CSAM on its platform, raising significant concerns about the safety of children using its services.\n\n---\n\n",
    "tweets": [
      "\ud83d\udea8 Apple is facing a lawsuit for allegedly abandoning its plans to detect child sexual abuse material (CSAM) on iCloud. Here's what you need to know about the implications of this decision. \ud83e\uddf5\ud83d\udc47 #Apple #CSAM #DataPrivacy",
      "1/ The lawsuit claims that Apple's failure to implement its previously announced CSAM detection system forces victims to relive their trauma. The case highlights a broader debate on balancing user privacy with the need to protect vulnerable individuals. [\ud83d\udd17 NYT](https://www.nytimes.com/2024/12/08/technology/apple-child-sexual-abuse-material-lawsuit.html)",
      "2/ Initially, Apple introduced a CSAM detection technology, NeuralHash, designed to identify known abusive content without compromising user privacy. However, privacy advocates raised concerns about potential government surveillance. [\ud83d\udd17 TechCrunch](https://techcrunch.com/2021/08/05/apple-icloud-photos-scanning/)",
      "3/ The current lawsuit stems from a 27-year-old woman, who, as a child, was a victim of abuse. She alleges that images of her trauma still circulate online, and Apple\u2019s inaction has perpetuated her suffering. [\ud83d\udd17 NYT](https://www.nytimes.com/2024/12/08/technology/apple-child-sexual-abuse-material-lawsuit.html)",
      "4/ Apple\u2019s spokesperson insists they are committed to combating CSAM while prioritizing user privacy. However, critics argue that their approach may not be sufficient, especially as they face increased scrutiny over their security measures. [\ud83d\udd17 NYT](https://www.nytimes.com/2024/12/08/technology/apple-child-sexual-abuse-material-lawsuit.html)",
      "5/ This isn\u2019t the first time Apple has faced legal action over CSAM. A previous lawsuit involved a 9-year-old girl, alleging that Apple failed to protect her from abuse via its iCloud platform. [\ud83d\udd17 TechCrunch](https://www.law.com/therecorder/2024/08/14/irreversible-suffering-apple-hit-with-class-action-over-lack-of-safety-guardrails-for-children/?slreturn=20241208141809)",
      "6/ Critics of Apple\u2019s NeuralHash technology fear it could lead to wrongful accusations or government misuse, raising ethical questions about how to effectively combat CSAM while safeguarding user privacy. [\ud83d\udd17 TechCrunch](https://techcrunch.com/2021/08/18/apples-csam-detection-tech-is-under-fire-again/)",
      "7/ Apple\u2019s dilemma illustrates the ongoing challenge for tech companies: How can they protect users from harm while adhering to privacy principles? As this lawsuit unfolds, the industry may need to rethink its strategies.",
      "\ud83d\udcac What are your thoughts on Apple's decision and the implications for user privacy and safety? Share your insights below! Don\u2019t forget to follow for more updates on tech, data privacy, and AI trends. #TechNews #DataEthics"
    ]
  },
  "status": "success"
}