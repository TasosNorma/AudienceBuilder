Getting Started with Scrapfly
Discover how to use Scrapfly API - the basics, available parameters and features, error handling and other information related to the API use.

On Steroids 
Smart defaults - scrape without being blocked  . Scrapfly pre-configures user-agent and other request headers.
Anti Scraping Protection feature bypasses all anti-scraping systems.
By default, the API responds in JSON. Though, a more efficient msgpack format is also available by setting the accept: application/msgpack header.
Text content is returned as utf-8 while binary is encoded in base64, so you can scrape any kind of data (pdf, zip, etc)
Gzip compression is available through content-encoding: gzip header.
Ability to debug and replay scrape requests from the dashboard log page and API.
Handle large payload, large text response greater than 5MB are called "CLOB" (Character Large Object) and binary are called "BLOB" (Binary Large Object) and can be downloaded separately with streaming support.
Quality of Life 
All scrape requests and metadata are automatically tracked on a Web Dashboard 
Multi project/scraper support through Project Management 
Experiment with the Visual API playground 
Status page with notification subscription.
Full API transparency through useful meta headers:
X-Scrapfly-Api-Cost API Cost billed
X-Scrapfly-Remaining-Api-Credit Remaining Api Credit, if 0, billed in extra credit
X-Scrapfly-Account-Concurrent-Usage You current concurrency usage of your account
X-Scrapfly-Account-Remaining-Concurrent-Usage Maximum concurrency allowed by the account
X-Scrapfly-Project-Concurrent-Usage Concurrency usage of the project
X-Scrapfly-Project-Remaining-Concurrent-Usage If the concurrency limit is set on the project otherwise equal to the account concurrency
Concurrency is defined by your subscription
Billing 
Scrapfly uses a credit system to bill scrape API requests where each scrape request has a variable cost based on:

Enabled scrape features and options (browser rendering, blocking bypass etc.).
Response body type (binary vs text results).
ASP feature can override scrape config details to bypass blocking which can alter the overall cost.
For more information see scrape API billing breakdown.

Billing is reported in every scrape response and the monitoring dashboard and can be controlled through Scrapfly budget settings. For more see Web Scraper Billing.

Handle Large Object
Large object CLOB for text and BLOB are offloaded from the API response to prevent any CPU/RAM issue with your JSON/MSGPACK decoder and increase the efficiency of your scrapers.

Instead of the actual content in response.result.content, you get an URL to download the large object. The URL is valid until the log expire.

response.result.format indicate whether it's a large object by checking if it's blob or clob
response.result.content contains the url to download the content. This url need to be authenticated with your API Key (Must be the API key that belong to project/env)
BLOB is not base64 encoded like binary format, you directly retrieve the binary data and the Content-Type header announce the actual type
Errors 
Scrapfly uses conventional HTTP response codes to indicate the success or failure of an API request.

Codes in the 2xx range indicate success.

Codes in the 4xx range indicate an error that failed given the information provided (e.g., a required parameter was omitted, not permitted, max concurrency reached, etc.).

Codes in the 5xx range indicate an error with Scrapfly's servers.

HTTP 422 - Request Failed provide extra headers in order to help as much as possible:

X-Scrapfly-Reject-Code: Error Code
X-Scrapfly-Reject-Description: URL to the related documentation
X-Scrapfly-Reject-Retryable: Indicate if the scrape is retryable
It is important to properly handle HTTP client errors in order to access the error headers and body. These details contain valuable information for troubleshooting, resolving the issue or reaching the support.
 HTTP Status Code Summary
200 - OK	Everything worked as expected.
400 - Bad Request	The request was unacceptable, often due to missing a required parameter or a bad value or a bad format.
401 - Unauthorized	No valid API key provided.
402 - Payment Required	A payment issue occur and need to be resolved
403 - Forbidden	The API key doesn't have permissions to perform the request.
422 - Request Failed	The parameters were valid but the request failed.
429 - Too Many Requests	All free quota used or max allowed concurrency or domain throttled
500, 502, 503 - Server Errors	Something went wrong on Scrapfly's end.
504 - Timeout	The scrape have timeout
You can check out the full error list to learn more.
Specification
Scrapfly has loads of features and the best way to discover them is through the specification docs below.

If you have any questions you can check out the Frequently asked question section or see the support chat.

By default, the API has a read timeout of 155 seconds. To avoid read timeout errors, you must configure your HTTP client to set the read timeout to 155 seconds. If you need a different timeout value, please refer to the documentation for information on how to control the timeout.
Try out the API directly in your terminal using curl:

GET
POST
PUT
PATCH
HEAD
OPTIONS
curl -X GET https://api.scrapfly.io/scrape?url=https://httpbin.dev/anything?q=I%20want%20to%20Scrape%20this&country=us&render_js=true&key=scp-test-ca84a5ef68c840e4a2ee3a89e4986860
 
Want to try out the API without coding? Check out our visual API player and test/generate code to use our API.

overview page of web interface for Scrapfly web API player

The default response format is JSON, and the scraped content is available in result.content. Your scrape configuration is present in config, and other activated feature information is available in context. To get the HTML page directly, refer to the proxified_response parameter.

HTTP Parameter
Description
Example
filter
url
required
Target URL to scrape. Must be URL encoded 
url=https://httpbin.dev/anything?q=I%20want%20to%20Scrape%20this
key
required
API Key to authenticate the call. You can find your key on your dashboard.
key=16eae084cff64841be193a95fc8fa67dso
proxy_pool
 popular
public_datacenter_pool
Select the proxy pool. A proxy pool is a network of proxy grouped by quality range and network type. The price varies based on the pool used.
proxy_pool=public_datacenter_pool proxy_pool=public_residential_pool
headers
popular
default: []
Pass custom headers to the request. See more on request customization. Must be url encoded 
headers[content-type]=application%2Fjson
headers[Cookie]=test%3D1%3Bauth%3D1
country
popular
Proxy country location - If not set it chooses a random location available. A reference to a country must be ISO 3166 alpha-2 (2 letters). The available countries are defined by the proxy pool you use.
country=us
country=us,ca,mx
country=us:1,ca:5,mx:3,-gb
country=-gb
lang
popular
default: proxy location
Select page language. By default it uses the language of the selected proxy location. Behind the scenes, it configures the Accept-Language HTTP header. If the website support the language, the content will be in that lang. Note: you cannot set headers Accept-Language header manually.
lang=en
lang=ch-FR,fr-FR,en
lang=en-IN,en-US
os
default: null
Operating System, if not selected it's random. Note: you cannot set os parameter and User-Agent header at the same time.
os=win11
os=mac
os=linux
os=chromeos
timeout
default: 150000
Timeout is expressed in milliseconds. It represents the maximum time allowed for Scrapfly to perform the scrape. Since timeout is not trivial to understand see our extended documentation on timeouts
timeout=30000
timeout=120000
format
default: raw
By default the format is the raw content of the scraped content, e.g: you scrape a JSON, you will get the JSON content, you scrape an HTML page you get an HTML page, the content is left untouched.

Clean HTML format is the HTML content cleaned from all the scripts, styles, and other non-essential tags. Relatives URLs are converted to absolute URLs, iframe are reintegrated in the content (listed API response response.result.iframe)

JSON format is split into 2 keys: metadata and content, where metadata contains all the embedded metadata from HTML markup (microdata, json-ld, microformats, etc). The content is parsed and cleaned then converted in JSON.

Markdown format supports the following options:
no_links, substitute links by their anchor
no_images, substitute images by their images
only_content, ignore all navigation, footer, headers, menu etc and return the content only
format=raw
format=textLLM Ready
format=markdownLLM Ready
format=markdown:no_links,no_imagesLLM Ready
format=clean_html
format=json
retry
default: true
Improve reliability with retries on failure (network, upstream http code >= 500, etc.). Note that the retry parameter has an impact on timeout.
retry=true
retry=false
proxified_response
popular
default: false
By default the API responds with a JSON response and the content is located in the result.content key. With proxified_response enabled the content of the page is directly returned as body and status code / headers are replaced with the response values. If you use a custom format like json or markdown, the content type will be altered to match the selected format.

When using data extraction features, the extracted data and the related content-type are located in result.extracted_data.
proxified_response=true
proxified_response=false
debug
default: false
Store the API result and take a screenshot if the rendering js is enabled. A sharable link with the saved response is available. Enable this when communicating with our support.
debug=true
debug=false
correlation_id
default: null
Helper ID for correlating a group of scrapes issued by the same worker or machine. You can use it as a filter in our monitoring dashboard.
correlation_id=e3ba784cde0d
tags
default: []
Add tags to your scrapes to group them which then can be filtered for in the dashboard.
tags[]=jewelery
tags[]=price
dns
default: false
Query and retrieve target DNS information
dns=true
dns=false
ssl
default: false
Pull remote SSL certificate and collect other TLS information. Only available for https://. Note that this parameter is not required to scrape https:// targets - it's for collecting SSL data.
ssl=true
ssl=false
webhook_name
popular
default: false
Queue you scrape request and redirect API response to a provided webhook endpoint. You can create a webhook endpoint from your dashboard, it takes the name of the webhook. Webhooks are scoped to the given project/env.
webhook_name=my-webhook-name
Data Extraction
extraction_template
new
default: null
Define an extraction template to get structured data.

Use an ephemeral template (declared on the fly on the API call) or a stored template (declared in the dashboard) by using the template name.
extraction_template=ephemeral:base64(json_template)
extraction_prompt
new
default: null
Instruction to extract data or ask question on the scraped content with an LLM (Large Language Model). Must be url encoded 
extraction_prompt=Summarize this document
extraction_model
new
default: null
AI Extraction to auto parse the scraped document to get structured data.
extraction_model=product
Anti Scraping ProtectionRequires asp to be enabled
asp
 popular
default: false
Anti Scraping Protection - Unblock protected website and bypass protection
asp=true
asp=false
cost_budget
default: null
ASP dynamically retry and upgrade some parameters (such as proxy_pool, browser) to pass and this changes dynamically the cost of the call, to make it more predictable, you can define a budget to respect. Make sure to set the minimum required to pass your target or the call will be rejected without even trying.
cost_budget=25
cost_budget=55
Headless Browser / Javascript RenderingAll related parameters require render_js enabled
render_js
 popular
default: false
Enable browser rendering. Scrape the target with a browser and render the page
render_js=true
render_js=false
rendering_wait
default: 1000
Delay in milliseconds to wait after the page was loaded.

Only executed on HTML pages
rendering_wait=5000
wait_for_selector
popular
default: null
Await until given selector is present.
CSS Selector
XPATH Selector
XHR Pattern


XHR pattern support prefix matching and wildcard matching. To use XHR pattern, prefix the selector with xhr:

Only executed on HTML pages
wait_for_selector=body
wait_for_selector=input[type="submit"]
wait_for_selector=//button[contains(text(),"Go")]
wait_for_selector=xhr:/page/rewiews
wait_for_selector=xhr:/page/*
js
default: null
Script to execute in the page. Must be base64 encoded with url safe option. 
If wait_for_selector is defined, the script is executed after and only if the selector is present.
If you need to wait or ensure you return data, use javascript await to prevent early return
Only executed on HTML pages
js=cmV0dXJuIG5hdmlnYXRvci51c2VyQWdlbnQ
screenshots
popular
default: []
Take screenshots of whole page or specific areas. You can take multiple screenshots of different areas:
fullpage - will take all page
css or xpath selector - will capture only the element.
The key argument is the name of the screenshot and the value is the selector or fullpage.
screenshots[page]=fullpage
screenshots[price]=#price
screenshot_flags
new
default: []
Screenshot flags to customize the screenshot behavior. You can set the quality, format, and more. Here are the available flags:
load_images Load images - Check dedicated documentation, extra cost is applied based on extra bandwidth
dark_mode Enable dark mode display
block_banners Block cookies banners and overlay that cover the screen
high_quality No compression on the output image
print_media_format Render the page in the print mode
screenshot_flags=load_images
screenshot_flags=load_images,block_banners,high_quality
js_scenario
default: null
User scenario to navigate or perform action on the page. It's a JSON and must be base64 encoded with url safe option. 
js_scenario=eydjbGljayc6IHsnc2VsZWN0b3InOiAnI3N1Ym1pdCd9fQ
geolocation
default: null
Grant geolocation permission and spoof latitude and longitude. Format: latitude,longitude
geolocation=48.856614,2.3522219
geolocation=-74.005941,40.712784
auto_scroll
default: false
Auto Scroll to the bottom of the page. It allows to trigger javascript rendering based on the user viewport intersection.
auto_scroll=true
auto_scroll=false
rendering_stage
default: complete
Stage to wait when rendering the page, You can choose between complete which is the default, or domcontentloaded if you want a fast render without waiting the full rendering (faster scrape)
rendering_stage=complete rendering_stage=domcontentloaded
CacheAll related parameters require cache enabled
cache
popular
default: false
Enable the cache layer. If the cache is MISS the scrape is performed otherwise the cached content is returned. If the TTL is expired, the cache will be refreshed by scraping again the target.
cache=true
cache=false
cache_ttl
default: 86400
Cache Time To Live in seconds. If you do a scrape after the TTL has expired, the target will be scraped and the cache refreshed with the new data.
cache_ttl=60
cache_ttl=3600
cache_clear
default: false
Force to refresh the cache and ensure the scrape is performed to cache the new version
cache_clear=true
cache_clear=false
SessionAll related parameters require session enabled
session
popular
default: null
Session name to be reused - Automatically store and restore cookies, fingerprint and proxy across many scrapes. Must be alphanumeric, max length is 255 characters.
session=17013313
session_sticky_proxy
default: true
Best effort is made to reuse the same proxy ip
session_sticky_proxy=true
session_sticky_proxy=false